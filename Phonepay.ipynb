{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bW0bS8T3n5g",
        "outputId": "d7fd4c13-ff7d-4f60-81e1-db30bc017bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pulse'...\n",
            "remote: Enumerating objects: 17904, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 17904 (delta 19), reused 17 (delta 17), pack-reused 17855 (from 2)\u001b[K\n",
            "Receiving objects: 100% (17904/17904), 26.13 MiB | 15.67 MiB/s, done.\n",
            "Resolving deltas: 100% (8723/8723), done.\n",
            "Updating files: 100% (9029/9029), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PhonePe/pulse.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**State List**"
      ],
      "metadata": {
        "id": "u4bekaq63ygt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path=\"/content/pulse/data/aggregated/transaction/country/india/state/\"\n",
        "Agg_state_list=os.listdir(path)\n",
        "Agg_state_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICvNqu6a319X",
        "outputId": "e2148cb2-bd83-4fc5-974b-bb44f4566806"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tripura',\n",
              " 'nagaland',\n",
              " 'chandigarh',\n",
              " 'andaman-&-nicobar-islands',\n",
              " 'maharashtra',\n",
              " 'odisha',\n",
              " 'rajasthan',\n",
              " 'bihar',\n",
              " 'delhi',\n",
              " 'haryana',\n",
              " 'gujarat',\n",
              " 'punjab',\n",
              " 'meghalaya',\n",
              " 'assam',\n",
              " 'jharkhand',\n",
              " 'himachal-pradesh',\n",
              " 'mizoram',\n",
              " 'manipur',\n",
              " 'chhattisgarh',\n",
              " 'telangana',\n",
              " 'lakshadweep',\n",
              " 'tamil-nadu',\n",
              " 'uttar-pradesh',\n",
              " 'dadra-&-nagar-haveli-&-daman-&-diu',\n",
              " 'andhra-pradesh',\n",
              " 'karnataka',\n",
              " 'sikkim',\n",
              " 'kerala',\n",
              " 'goa',\n",
              " 'uttarakhand',\n",
              " 'jammu-&-kashmir',\n",
              " 'west-bengal',\n",
              " 'puducherry',\n",
              " 'madhya-pradesh',\n",
              " 'ladakh',\n",
              " 'arunachal-pradesh']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Json Code**"
      ],
      "metadata": {
        "id": "r2V9VnB55Si1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Your JSON string (without comments)\n",
        "data_str = \"\"\"\n",
        "{\n",
        "    \"success\": true,\n",
        "    \"code\": \"SUCCESS\",\n",
        "    \"data\": {\n",
        "        \"states\": [\n",
        "            {\n",
        "                \"entityName\": \"karnataka\",\n",
        "                \"metric\": {\n",
        "                    \"type\": \"TOTAL\",\n",
        "                    \"count\": 523797492,\n",
        "                    \"amount\": 7.549953574123948E11\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        \"districts\": [\n",
        "            {\n",
        "                \"entityName\": \"bengaluru urban\",\n",
        "                \"metric\": {\n",
        "                    \"type\": \"TOTAL\",\n",
        "                    \"count\": 348712787,\n",
        "                    \"amount\": 4.324013412317671E11\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        \"pincodes\": [\n",
        "            {\n",
        "                \"entityName\": \"560001\",\n",
        "                \"metric\": {\n",
        "                    \"type\": \"TOTAL\",\n",
        "                    \"count\": 111898471,\n",
        "                    \"amount\": 1.5427512629157785E11\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"responseTimestamp\": 1630346629360\n",
        "}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "5D_J0QHz5VE9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert json to python dict**"
      ],
      "metadata": {
        "id": "0MIsdMiu47JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert JSON string to Python dictionary\n",
        "data = json.loads(data_str)\n",
        "\n",
        "# Example: Extract states key-values\n",
        "print(\"States:\")\n",
        "for state in data[\"data\"][\"states\"]:\n",
        "    for key, value in state.items():\n",
        "        print(f\"Key: {key} -> Value: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti1T9vZ85CMR",
        "outputId": "18776c2c-a076-4e25-c035-e8cf8ec20873"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "States:\n",
            "Key: entityName -> Value: karnataka\n",
            "Key: metric -> Value: {'type': 'TOTAL', 'count': 523797492, 'amount': 754995357412.3948}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aggrated Code**"
      ],
      "metadata": {
        "id": "gF0BlALc8nYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "#This is to direct the path to get the data as states\n",
        "\n",
        "path=\"/content/pulse/data/aggregated/insurance/country/india/state/\"\n",
        "Agg_state_list=os.listdir(path)\n",
        "Agg_state_list\n",
        "#Agg_state_list--> to get the list of states in India\n",
        "\n",
        "#<------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------>#\n",
        "\n",
        "#This is to extract the data's to create a dataframe\n",
        "\n",
        "Ins={'State':[], 'Year':[],'Quater':[],'Transacion_type':[], 'Transacion_count':[], 'Transacion_amount':[]}\n",
        "\n",
        "for i in Agg_state_list:\n",
        "    p_i=path+i+\"/\"\n",
        "    Agg_yr=os.listdir(p_i)\n",
        "    for j in Agg_yr:\n",
        "        p_j=p_i+j+\"/\"\n",
        "        Agg_yr_list=os.listdir(p_j)\n",
        "        for k in Agg_yr_list:\n",
        "            p_k=p_j+k\n",
        "            Data=open(p_k,'r')\n",
        "            D=json.load(Data)\n",
        "            for z in D['data']['transactionData']:\n",
        "              Name=z['name']\n",
        "              count=z['paymentInstruments'][0]['count']\n",
        "              amount=z['paymentInstruments'][0]['amount']\n",
        "              Ins['Transacion_type'].append(Name)\n",
        "              Ins['Transacion_count'].append(count)\n",
        "              Ins['Transacion_amount'].append(amount)\n",
        "              Ins['State'].append(i)\n",
        "              Ins['Year'].append(j)\n",
        "              Ins['Quater'].append(int(k.strip('.json')))\n",
        "#Succesfully created a dataframe\n",
        "Agg_insurance=pd.DataFrame(Ins)\n",
        "Agg_insurance.shape\n",
        "print(Agg_insurance)"
      ],
      "metadata": {
        "id": "_7oXHNid8qOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aggreated Transaction**"
      ],
      "metadata": {
        "id": "nTquxDmb8405"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Once created the clone of GIT-HUB repository then,\n",
        "#Required libraries for the program\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "#This is to direct the path to get the data as states\n",
        "\n",
        "path=\"/content/pulse/data/aggregated/transaction/country/india/state/\"\n",
        "Agg_state_list=os.listdir(path)\n",
        "Agg_state_list\n",
        "#Agg_state_list--> to get the list of states in India\n",
        "\n",
        "#<------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------>#\n",
        "\n",
        "#This is to extract the data's to create a dataframe\n",
        "\n",
        "Trans={'State':[], 'Year':[],'Quater':[],'Transacion_type':[], 'Transacion_count':[], 'Transacion_amount':[]}\n",
        "\n",
        "for i in Agg_state_list:\n",
        "    p_i=path+i+\"/\"\n",
        "    Agg_yr=os.listdir(p_i)\n",
        "    for j in Agg_yr:\n",
        "        p_j=p_i+j+\"/\"\n",
        "        Agg_yr_list=os.listdir(p_j)\n",
        "        for k in Agg_yr_list:\n",
        "            p_k=p_j+k\n",
        "            Data=open(p_k,'r')\n",
        "            D=json.load(Data)\n",
        "            for z in D['data']['transactionData']:\n",
        "              Name=z['name']\n",
        "              count=z['paymentInstruments'][0]['count']\n",
        "              amount=z['paymentInstruments'][0]['amount']\n",
        "              Trans['Transacion_type'].append(Name)\n",
        "              Trans['Transacion_count'].append(count)\n",
        "              Trans['Transacion_amount'].append(amount)\n",
        "              Trans['State'].append(i)\n",
        "              Trans['Year'].append(j)\n",
        "              Trans['Quater'].append(int(k.strip('.json')))\n",
        "#Succesfully created a dataframe\n",
        "Agg_Trans=pd.DataFrame(Trans)\n",
        "print(Agg_Trans.to_string())\n"
      ],
      "metadata": {
        "id": "Y1Jhix4F8805"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aggreated User**"
      ],
      "metadata": {
        "id": "YiSNmldq-IYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Set path to data\n",
        "path = \"/content/pulse/data/aggregated/user/country/india/state/\"\n",
        "Agg_state_list = os.listdir(path)\n",
        "\n",
        "#  Define the dictionary OUTSIDE the loop\n",
        "user = {\n",
        "    'State': [],\n",
        "    'Year': [],\n",
        "    'Quater': [],\n",
        "    'Brand_type': [],\n",
        "    'Registered_count': [],\n",
        "    'Percentage_Userdevices': []\n",
        "}\n",
        "\n",
        "# Loop through all JSON files and extract data\n",
        "for i in Agg_state_list:\n",
        "    p_i = path + i + \"/\"\n",
        "    Agg_yr = os.listdir(p_i)\n",
        "\n",
        "    for j in Agg_yr:\n",
        "        p_j = p_i + j + \"/\"\n",
        "        Agg_yr_list = os.listdir(p_j)\n",
        "\n",
        "        for k in Agg_yr_list:\n",
        "            p_k = p_j + k\n",
        "            with open(p_k, 'r') as Data:\n",
        "                D = json.load(Data)\n",
        "\n",
        "                if D['data']['usersByDevice'] is not None:\n",
        "                    for z in D['data']['usersByDevice']:\n",
        "                        Name = z.get('brand', 'Unknown')\n",
        "                        count = z.get('count', 0)\n",
        "                        Percentage = z.get('percentage', 0.0)\n",
        "\n",
        "                        user['Brand_type'].append(Name)\n",
        "                        user['Registered_count'].append(count)\n",
        "                        user['Percentage_Userdevices'].append(Percentage)\n",
        "                        user['State'].append(i)\n",
        "                        user['Year'].append(j)\n",
        "                        user['Quater'].append(int(k.strip('.json')))\n",
        "\n",
        "#  Create DataFrame\n",
        "Agg_user = pd.DataFrame(user)\n",
        "Agg_user.shape\n",
        "\n",
        "# Optional: Preview the data\n",
        "print(Agg_user.head())\n"
      ],
      "metadata": {
        "id": "hh8W4JOI-LIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAP DATAFRAME**"
      ],
      "metadata": {
        "id": "TRmWie6s-VkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ Set your path to the correct directory\n",
        "path = \"/content/pulse/data/map/insurance/hover/country/india/state/\"\n",
        "\n",
        "Map_Insurance = {\n",
        "    'State': [], 'Year': [], 'Quarter': [],\n",
        "    'District': [], 'User_count': [], 'User_amount': []\n",
        "}\n",
        "\n",
        "# Loop through each state folder\n",
        "for state in os.listdir(path):\n",
        "    state_path = os.path.join(path, state)\n",
        "    if not os.path.isdir(state_path):\n",
        "        continue\n",
        "\n",
        "    for year in os.listdir(state_path):\n",
        "        year_path = os.path.join(state_path, year)\n",
        "        if not os.path.isdir(year_path):\n",
        "            continue\n",
        "\n",
        "        for quarter_file in os.listdir(year_path):\n",
        "            if not quarter_file.endswith(\".json\"):\n",
        "                continue\n",
        "\n",
        "            quarter = quarter_file.strip(\".json\")\n",
        "            file_path = os.path.join(year_path, quarter_file)\n",
        "\n",
        "            try:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                # ✅ Correct key: hoverDataList (not hoverData)\n",
        "                hover_data_list = data.get(\"data\", {}).get(\"hoverDataList\", [])\n",
        "\n",
        "                for entry in hover_data_list:\n",
        "                    district = entry.get(\"name\", \"Unknown\")\n",
        "                    metric = entry.get(\"metric\", [])\n",
        "\n",
        "                    if metric and isinstance(metric[0], dict):\n",
        "                        count = metric[0].get(\"count\", 0)\n",
        "                        amount = metric[0].get(\"amount\", 0.0)\n",
        "                    else:\n",
        "                        count = 0\n",
        "                        amount = 0.0\n",
        "\n",
        "                    Map_Insurance['State'].append(state)\n",
        "                    Map_Insurance['Year'].append(year)\n",
        "                    Map_Insurance['Quarter'].append(int(quarter))\n",
        "                    Map_Insurance['District'].append(district)\n",
        "                    Map_Insurance['User_count'].append(count)\n",
        "                    Map_Insurance['User_amount'].append(amount)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error reading {file_path}: {e}\")\n",
        "\n",
        "# ✅ Create DataFrame\n",
        "Map_Insurance_df = pd.DataFrame(Map_Insurance)\n",
        "\n",
        "# ✅ Print sample output\n",
        "print(\"✅ DataFrame Shape:\", Map_Insurance_df.shape)\n",
        "print(Map_Insurance_df.head())\n",
        "\n",
        "# Optional: Save to CSV\n",
        "Map_Insurance_df.to_csv(\"Map_Insurance_Data.csv\", index=False)\n",
        "Map_Insurance_df\n"
      ],
      "metadata": {
        "id": "fPuDlNs2-X3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Map Transaction**"
      ],
      "metadata": {
        "id": "S0oEolaA-qhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "path = \"/content/pulse/data/map/transaction/hover/country/india/state/\"\n",
        "\n",
        "#  Create a dictionary to store extracted data\n",
        "Map_Transaction = {\n",
        "    'State': [], 'Year': [], 'Quarter': [],\n",
        "    'District': [], 'User_count': [], 'User_amount': []\n",
        "}\n",
        "\n",
        "#  Loop through all folders and files\n",
        "for state in os.listdir(path):\n",
        "    state_path = os.path.join(path, state)\n",
        "    if not os.path.isdir(state_path):\n",
        "        continue\n",
        "\n",
        "    for year in os.listdir(state_path):\n",
        "        year_path = os.path.join(state_path, year)\n",
        "        if not os.path.isdir(year_path):\n",
        "            continue\n",
        "\n",
        "        for quarter_file in os.listdir(year_path):\n",
        "            if not quarter_file.endswith(\".json\"):\n",
        "                continue\n",
        "\n",
        "            file_path = os.path.join(year_path, quarter_file)\n",
        "            quarter = quarter_file.strip(\".json\")\n",
        "\n",
        "            try:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                # ✅ Get hoverDataList\n",
        "                hover_list = data.get(\"data\", {}).get(\"hoverDataList\", [])\n",
        "\n",
        "                for entry in hover_list:\n",
        "                    district = entry.get(\"name\", \"Unknown\")\n",
        "                    metrics = entry.get(\"metric\", [])\n",
        "\n",
        "                    if metrics and isinstance(metrics[0], dict):\n",
        "                        count = metrics[0].get(\"count\", 0)\n",
        "                        amount = metrics[0].get(\"amount\", 0.0)\n",
        "                    else:\n",
        "                        count = 0\n",
        "                        amount = 0.0\n",
        "\n",
        "                    # Append to dictionary\n",
        "                    Map_Transaction['State'].append(state)\n",
        "                    Map_Transaction['Year'].append(year)\n",
        "                    Map_Transaction['Quarter'].append(int(quarter))\n",
        "                    Map_Transaction['District'].append(district)\n",
        "                    Map_Transaction['User_count'].append(count)\n",
        "                    Map_Transaction['User_amount'].append(amount)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Error reading {file_path}: {e}\")\n",
        "\n",
        "#  Create DataFrame\n",
        "Map_Transaction_df = pd.DataFrame(Map_Transaction)\n",
        "\n",
        "#  Print output\n",
        "print(\"\\n DataFrame Shape:\", Map_Transaction_df.shape)\n",
        "print(\"\\n Sample Records:\")\n",
        "print(Map_Transaction_df.head().to_string(index=False))\n"
      ],
      "metadata": {
        "id": "mnIrskoL-ttd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Map User**"
      ],
      "metadata": {
        "id": "45w3I7JB-6rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/pulse/data/map/user/hover/country/india/state/\"\n",
        "\n",
        "#  Create a dictionary to store extracted data\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# ✅ Create dictionary for final DataFrame\n",
        "Map_Users = {\n",
        "    'State': [], 'Year': [], 'Quarter': [],\n",
        "    'District': [], 'RegisteredUsers': [], 'AppOpens': []\n",
        "}\n",
        "\n",
        "# ✅ Traverse all state/year/quarter JSONs\n",
        "for state in os.listdir(path):\n",
        "    state_path = os.path.join(path, state)\n",
        "    if not os.path.isdir(state_path):\n",
        "        continue\n",
        "\n",
        "    for year in os.listdir(state_path):\n",
        "        year_path = os.path.join(state_path, year)\n",
        "        if not os.path.isdir(year_path):\n",
        "            continue\n",
        "\n",
        "        for quarter_file in os.listdir(year_path):\n",
        "            if not quarter_file.endswith(\".json\"):\n",
        "                continue\n",
        "\n",
        "            quarter = quarter_file.strip(\".json\")\n",
        "            file_path = os.path.join(year_path, quarter_file)\n",
        "\n",
        "            try:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                # ✅ Access hoverData dictionary\n",
        "                hover_data = data.get(\"data\", {}).get(\"hoverData\", {})\n",
        "\n",
        "                for district, values in hover_data.items():\n",
        "                    registered = values.get(\"registeredUsers\", 0)\n",
        "                    opens = values.get(\"appOpens\", 0)\n",
        "\n",
        "                    Map_Users['State'].append(state)\n",
        "                    Map_Users['Year'].append(year)\n",
        "                    Map_Users['Quarter'].append(int(quarter))\n",
        "                    Map_Users['District'].append(district)\n",
        "                    Map_Users['RegisteredUsers'].append(registered)\n",
        "                    Map_Users['AppOpens'].append(opens)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error reading {file_path}: {e}\")\n",
        "\n",
        "# ✅ Convert to DataFrame\n",
        "Map_Users_df = pd.DataFrame(Map_Users)\n",
        "\n",
        "# ✅ Print result\n",
        "print(\"✅ DataFrame Shape:\", Map_Users_df.shape)\n",
        "print(\"\\n✅ Sample Data:\")\n",
        "print(Map_Users_df.head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "ANgZ__5Y--1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top Dataframe**"
      ],
      "metadata": {
        "id": "hJB3nRaK_F7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "path = \"/content/pulse/data/top/insurance/country/india/state/\"\n",
        "\n",
        "# Dictionary to collect all data\n",
        "all_data = {\n",
        "    'State': [], 'Year': [], 'Quarter': [],\n",
        "     'Entity_Name': [],\n",
        "     'Transaction_Count': [],\n",
        "     'Transaction_Amount': []\n",
        "}\n",
        "\n",
        "# Loop through all states\n",
        "states = os.listdir(path)\n",
        "\n",
        "for state in states:\n",
        "    state_path = os.path.join(path, state)\n",
        "\n",
        "    if not os.path.isdir(state_path):\n",
        "        continue\n",
        "\n",
        "    years = os.listdir(state_path)\n",
        "\n",
        "    for year in years:\n",
        "        year_path = os.path.join(state_path, year)\n",
        "\n",
        "        if not os.path.isdir(year_path):\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(year_path):\n",
        "            if file.endswith(\".json\"):\n",
        "                quarter = file.replace(\".json\", \"\")\n",
        "                file_path = os.path.join(year_path, file)\n",
        "\n",
        "                try:\n",
        "                    with open(file_path, \"r\") as f:\n",
        "                        data = json.load(f)\n",
        "\n",
        "                        for level in ['states', 'districts', 'pincodes']:\n",
        "                            level_data = data.get('data', {}).get(level, [])\n",
        "\n",
        "                            if not level_data:\n",
        "                                continue\n",
        "\n",
        "                            for item in level_data:\n",
        "                                all_data['State'].append(state)\n",
        "                                all_data['Year'].append(year)\n",
        "                                all_data['Quarter'].append(quarter)\n",
        "                                all_data['Entity_Name'].append(item.get('entityName'))\n",
        "                                all_data['Transaction_Count'].append(item.get('metric', {}).get('count', 0))\n",
        "                                all_data['Transaction_Amount'].append(item.get('metric', {}).get('amount', 0))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "TY5zgQzJ_Jd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top Transaction**"
      ],
      "metadata": {
        "id": "T5D00aYp_QSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Set the path to your folder\n",
        "path = \"/content/pulse/data/top/transaction/country/india/state/\"\n",
        "# Final data to collect everything\n",
        "all_data = {\n",
        "    'State': [], 'Year': [], 'Quarter': [],\n",
        "\n",
        "    'District_Name': [], 'District_Transactions': [], 'District_Amount': [],\n",
        "    'Pincode': [], 'Pincode_Transactions': [], 'Pincode_Amount': []\n",
        "}\n",
        "\n",
        "# Loop through all state folders\n",
        "for state in os.listdir(path):\n",
        "    state_path = os.path.join(path, state)\n",
        "    if not os.path.isdir(state_path):\n",
        "        continue\n",
        "\n",
        "    for year in os.listdir(state_path):\n",
        "        year_path = os.path.join(state_path, year)\n",
        "        if not os.path.isdir(year_path):\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(year_path):\n",
        "            if file.endswith(\".json\"):\n",
        "                quarter = file.replace(\".json\", \"\")\n",
        "                file_path = os.path.join(year_path, file)\n",
        "\n",
        "                try:\n",
        "                    with open(file_path, 'r') as f:\n",
        "                        raw = json.load(f)\n",
        "                        data = raw.get('data')\n",
        "\n",
        "                        if not data:\n",
        "                            print(f\"Skipping: {file_path} (no data)\")\n",
        "                            continue\n",
        "\n",
        "                        states = data.get(\"states\") or []\n",
        "                        districts = data.get(\"districts\") or []\n",
        "                        pincodes = data.get(\"pincodes\") or []\n",
        "\n",
        "                        max_len = max(len(states), len(districts), len(pincodes))\n",
        "\n",
        "\n",
        "                        for i in range(max_len):\n",
        "                            # State-level\n",
        "                            s_name = states[i]['entityName'] if i < len(states) else None\n",
        "                            s_txn = states[i]['metric']['count'] if i < len(states) else None\n",
        "                            s_amt = states[i]['metric']['amount'] if i < len(states) else None\n",
        "\n",
        "                            # District-level\n",
        "                            d_name = districts[i]['entityName'] if i < len(districts) else None\n",
        "                            d_txn = districts[i]['metric']['count'] if i < len(districts) else None\n",
        "                            d_amt = districts[i]['metric']['amount'] if i < len(districts) else None\n",
        "\n",
        "                            # Pincode-level\n",
        "                            p_name = pincodes[i]['entityName'] if i < len(pincodes) else None\n",
        "                            p_txn = pincodes[i]['metric']['count'] if i < len(pincodes) else None\n",
        "                            p_amt = pincodes[i]['metric']['amount'] if i < len(pincodes) else None\n",
        "\n",
        "                            all_data['State'].append(state)\n",
        "                            all_data['Year'].append(year)\n",
        "                            all_data['Quarter'].append(quarter)\n",
        "                            all_data['District_Name'].append(d_name)\n",
        "                            all_data['District_Transactions'].append(d_txn)\n",
        "                            all_data['District_Amount'].append(d_amt)\n",
        "                            all_data['Pincode'].append(p_name)\n",
        "                            all_data['Pincode_Transactions'].append(p_txn)\n",
        "                            all_data['Pincode_Amount'].append(p_amt)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# Print sample output\n",
        "print(df.head())\n",
        "\n",
        "# Optional: Save to CSV\n",
        "# df.to_csv(\"Phonepe_Top_Transactions.csv\", index=False)"
      ],
      "metadata": {
        "id": "o3mXWeAZ_OYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top User**"
      ],
      "metadata": {
        "id": "uS-0BqCq_YJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Set the path to your folder\n",
        "path = \"/content/pulse/data/top/user/country/india/state/\"\n",
        "\n",
        "\n",
        "all_data = {\n",
        "    'State': [], 'Year': [], 'Quarter': [],\n",
        "\n",
        "    'District_Name': [], 'District_RegisteredUsers': [],\n",
        "    'Pincode': [], 'Pincode_RegisteredUsers': []\n",
        "}\n",
        "\n",
        "# Loop through all state folders\n",
        "for state in os.listdir(path):\n",
        "    state_path = os.path.join(path, state)\n",
        "    if not os.path.isdir(state_path):\n",
        "        continue\n",
        "\n",
        "    for year in os.listdir(state_path):\n",
        "        year_path = os.path.join(state_path, year)\n",
        "        if not os.path.isdir(year_path):\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(year_path):\n",
        "            if file.endswith(\".json\"):\n",
        "                quarter = file.replace(\".json\", \"\")\n",
        "                file_path = os.path.join(year_path, file)\n",
        "\n",
        "                try:\n",
        "                    with open(file_path, 'r') as f:\n",
        "                        raw = json.load(f)\n",
        "                        data = raw.get('data')\n",
        "\n",
        "                        if not data:\n",
        "                            print(f\"Skipping: {file_path} (no data)\")\n",
        "                            continue\n",
        "\n",
        "                        states = data.get(\"states\") or []\n",
        "                        districts = data.get(\"districts\") or []\n",
        "                        pincodes = data.get(\"pincodes\") or []\n",
        "\n",
        "                        max_len = max(len(states), len(districts), len(pincodes))\n",
        "\n",
        "                        for i in range(max_len):\n",
        "                            # State-level\n",
        "                            s_name = states[i]['name'] if i < len(states) else None\n",
        "                            s_users = states[i]['registeredUsers'] if i < len(states) else None\n",
        "\n",
        "                            # District-level\n",
        "                            d_name = districts[i]['name'] if i < len(districts) else None\n",
        "                            d_users = districts[i]['registeredUsers'] if i < len(districts) else None\n",
        "\n",
        "                            # Pincode-level\n",
        "                            p_name = pincodes[i]['name'] if i < len(pincodes) else None\n",
        "                            p_users = pincodes[i]['registeredUsers'] if i < len(pincodes) else None\n",
        "\n",
        "                            all_data['State'].append(state)\n",
        "                            all_data['Year'].append(year)\n",
        "                            all_data['Quarter'].append(quarter)\n",
        "                            all_data['District_Name'].append(d_name)\n",
        "                            all_data['District_RegisteredUsers'].append(d_users)\n",
        "                            all_data['Pincode'].append(p_name)\n",
        "                            all_data['Pincode_RegisteredUsers'].append(p_users)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "print(df_registered_users.shape)\n",
        "df_registered_users = pd.DataFrame(all_data)\n",
        "df_registered_users\n",
        "\n"
      ],
      "metadata": {
        "id": "uIWVOAcY_cqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNAWrgbFANpx",
        "outputId": "4fb1ed49-c7bd-4586-981c-9c7f94d91ae8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.48.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pymysql"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVStuMtnAcZM",
        "outputId": "1981d293-9777-42f8-ae3a-d2725b265130"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**India Map Phonepay transaction**"
      ],
      "metadata": {
        "id": "nIxmNG_f_plJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import pymysql\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# --- Streamlit Page Config ---\n",
        "st.set_page_config(page_title=\"📍 PhonePe India Map\", layout=\"wide\")\n",
        "st.title(\"📍 PhonePe Top Transactions - India Map\")\n",
        "\n",
        "# Load GeoJSON for India states\n",
        "geojson_url = \"https://gist.githubusercontent.com/jbrobst/56c13bbbf9d97d187fea01ca62ea5112/raw/e388c4cae20aa53cb5090210a42ebb9b765c0a36/india_states.geojson\"\n",
        "india_states_geojson = requests.get(geojson_url).json()\n",
        "\n",
        "# --- Connect to MySQL ---\n",
        "connection = pymysql.connect(\n",
        "    host='localhost',\n",
        "    user='root',\n",
        "    password='12345',\n",
        "    database='Phonepay_DB'\n",
        ")\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT State, SUM(District_Amount) AS Total_Amount\n",
        "FROM top_transactions\n",
        "GROUP BY State\n",
        "\"\"\"\n",
        "df = pd.read_sql(query, connection)\n",
        "connection.close()\n",
        "\n",
        "# Optional: State name formatting if mismatches occur\n",
        "df['State'] = df['State'].str.title()\n",
        "\n",
        "# Plotly Choropleth\n",
        "fig = px.choropleth(\n",
        "    df,\n",
        "    geojson=india_states_geojson,\n",
        "    featureidkey='properties.ST_NM',\n",
        "    locations='State',\n",
        "    color='Total_Amount',\n",
        "    color_continuous_scale='ice',\n",
        "    title=\"📊 State-wise Transaction Amount\"\n",
        ")\n",
        "\n",
        "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
        "st.plotly_chart(fig, use_container_width=True)"
      ],
      "metadata": {
        "id": "a1eaN5z0_uU9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}